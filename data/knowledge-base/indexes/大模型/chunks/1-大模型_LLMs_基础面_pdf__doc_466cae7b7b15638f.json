[
  {
    "chunk_id": "chunk_a816347bd3a46157",
    "course_id": "course_9ff5901482019ab3",
    "source_doc_id": "doc_466cae7b7b15638f",
    "source_doc_name": "1-大模型（LLMs）基础面.pdf",
    "source_doc_type": "pdf",
    "title_path": "1-大模型（LLMs）基础面.pdf > 目前 主流的开源模型体系 有哪些? > 片段 1",
    "content": "目前 主流的开源模型体系 分三种:\n\n•第一种: prefixDecoder 系\n\n•介绍:输入双向注意力,输出单向注意力\n\n•代表模型: ChatGLM 、 ChatGLM2 、 U-PaLM\n\n•第二种: causalDecoder 系\n\n•介绍:从左到右的单向注意力\n\n•代表模型: LLaMA-7B 、 LLaMa 衍生物\n\n•第三种: Encoder-Decoder\n\n•介绍:输入双向注意力,输出单向注意力\n\n•代表模型: T5 、 Flan-T5 、 BART",
    "order_index": 1,
    "char_count": 237
  },
  {
    "chunk_id": "chunk_fbfc4b6539f69e01",
    "course_id": "course_9ff5901482019ab3",
    "source_doc_id": "doc_466cae7b7b15638f",
    "source_doc_name": "1-大模型（LLMs）基础面.pdf",
    "source_doc_type": "pdf",
    "title_path": "1-大模型（LLMs）基础面.pdf > prefixDecoder 和 causalDecoder 和 Encoder-Decoder 区别是什么? > 片段 2",
    "content": "prefixDecoder 和 causalDecoder 和 Encoder-Decoder 区别 在于 attentionmask 不同:\n\n•Encoder-Decoder :\n\n•在输入上采用双向注意力,对问题的编码理解更充分\n\n•适用任务:在偏理解的 NLP 任务上效果好\n\n•缺点:在长文本生成任务上效果差,训练效率低;\n\n•causalDecoder :\n\n•自回归语言模型,预训练和下游应用是完全一致的,严格遵守 只有后面的 token 才能看到前面的token的规则;\n\n•适用任务:文本生成任务效果好\n\n•优点:训练效率高, zero-shot 能力更强,具有涌现能力\n\n•prefixDecoder :\n\n•特点: prefix部分的 token 互相能看到 ,causalDecoder 和 Encoder-Decoder 折中;\n\n•缺点:训练效率低",
    "order_index": 2,
    "char_count": 388
  },
  {
    "chunk_id": "chunk_2576d6b0701ea7b4",
    "course_id": "course_9ff5901482019ab3",
    "source_doc_id": "doc_466cae7b7b15638f",
    "source_doc_name": "1-大模型（LLMs）基础面.pdf",
    "source_doc_type": "pdf",
    "title_path": "1-大模型（LLMs）基础面.pdf > 大模型 LLM 的 训练目标 是什么? > 片段 3",
    "content": "根据 已有词 预测下一个词,训练目标为最大似然函数:\n\n训练效率: PrefixDecoder < CausalDecoder。CausalDecoder 结构会在 所有 token 上计算损失,而 PrefixDecoder 只会在 输出上 计算损失。\n\n随机替换掉一些文本段,训练语言模型去恢复被打乱的文本段。目标函数为 :\n\n去噪自编码器的实现难度更高。采用去噪自编码器作为训练目标的任务有 GLM-130B 、 T5.",
    "order_index": 3,
    "char_count": 213
  },
  {
    "chunk_id": "chunk_f65abe0786d3f6f1",
    "course_id": "course_9ff5901482019ab3",
    "source_doc_id": "doc_466cae7b7b15638f",
    "source_doc_name": "1-大模型（LLMs）基础面.pdf",
    "source_doc_type": "pdf",
    "title_path": "1-大模型（LLMs）基础面.pdf > 涌现能力是啥原因? > 片段 4",
    "content": "根据前人分析和论文总结,大致是 2 个猜想:\n\n•复杂任务 vs 子任务,这个其实好理解,比如我们假设某个任务 T 有 5 个子任务 Sub-T 构成,每个 sub-T 随着模型增长,指标从 40% 提升到 60% ,但是最终任务的指标只从 1.1% 提升到了 7% ,也就是说宏观上看到了涌现现象,但是子任务效果其实是平滑增长的。",
    "order_index": 4,
    "char_count": 166
  },
  {
    "chunk_id": "chunk_26b95a73288f40d7",
    "course_id": "course_9ff5901482019ab3",
    "source_doc_id": "doc_466cae7b7b15638f",
    "source_doc_name": "1-大模型（LLMs）基础面.pdf",
    "source_doc_type": "pdf",
    "title_path": "1-大模型（LLMs）基础面.pdf > 为何现在的大模型大部分是 Decoderonly 结构? > 片段 5",
    "content": "因为decoder-only 结构模型在没有任何微调数据的情况下, zero-shot 的表现能力最好 。而encoder-decoder 则需要在一定量的标注数据上做 multitask-finetuning 才能够激发最佳性能 。 目前的 LargeLM 的训练范式还是在大规模语料上做自监督学习,很显然 zero-shot 性能更好的 decoder-only 架构才能更好的利用这些无标注的数据。",
    "order_index": 5,
    "char_count": 202
  },
  {
    "chunk_id": "chunk_cccc727a0807473a",
    "course_id": "course_9ff5901482019ab3",
    "source_doc_id": "doc_466cae7b7b15638f",
    "source_doc_name": "1-大模型（LLMs）基础面.pdf",
    "source_doc_type": "pdf",
    "title_path": "1-大模型（LLMs）基础面.pdf > 为何现在的大模型大部分是 Decoderonly 结构? > 片段 6",
    "content": "大模型使用 decoder-only 架构除了训练效率和工程实现上的优势外,在理论上因为 Encoder 的双向注意力会存在低秩的问题,这可能会削弱模型的表达能力 。就生成任务而言,引入双向注意力并无实质的好处。而 Encoder-decoder模型架构之所以能够在某些场景下表现更好,大概是因为它多了一倍参数。所以在同等参数量、同等推理成本下, Decoder-only 架构就是最优的选择了。",
    "order_index": 6,
    "char_count": 198
  },
  {
    "chunk_id": "chunk_f168a2d64cf71c1d",
    "course_id": "course_9ff5901482019ab3",
    "source_doc_id": "doc_466cae7b7b15638f",
    "source_doc_name": "1-大模型（LLMs）基础面.pdf",
    "source_doc_type": "pdf",
    "title_path": "1-大模型（LLMs）基础面.pdf > 简单 介绍一下 大模型【 LLMs 】? > 片段 7",
    "content": "大模型:一般指 1亿以上参数的模型 ,但是这个标准一直在升级,目前万亿参数以上的模型也有了。大语言模型 (LargeLanguageModel , LLM )是针对语言的大模型。\n\n1.语言模型1.去噪自编码器\n\n大模型【 LLMs 】后面跟的 175B 、 60B 、 540B 等 指参数的个数, B 是 Billion/ 十亿的意思, 175B 是 1750 亿参数,这是 ChatGPT 大约的参数规模。",
    "order_index": 7,
    "char_count": 206
  },
  {
    "chunk_id": "chunk_a0e3715f34dc3c13",
    "course_id": "course_9ff5901482019ab3",
    "source_doc_id": "doc_466cae7b7b15638f",
    "source_doc_name": "1-大模型（LLMs）基础面.pdf",
    "source_doc_type": "pdf",
    "title_path": "1-大模型（LLMs）基础面.pdf > 大模型【 LLMs 】具有什么优点? > 片段 8",
    "content": "1.可以完成多种自然语言处理任务。这种预训练和微调的方法可以减少数据标注的成本和时间,提高模型的泛化能力;\n\n2.可以利用生成式人工智能技术来产生新颖和有价值的内容,例如图像、文本、音乐等。这种生成能力可以帮助用户在创意、娱乐、教育等领域获得更好的体验和效果;\n\n3.可以利用涌现能力( EmergentCapabilities )来完成一些之前无法完成或者很难完成的任务,例如数学应用题、常识推理、符号操作等。这种涌现能力可以反映模型的智能水平和推理能力。",
    "order_index": 8,
    "char_count": 229
  },
  {
    "chunk_id": "chunk_a8a8dca308b9c6ba",
    "course_id": "course_9ff5901482019ab3",
    "source_doc_id": "doc_466cae7b7b15638f",
    "source_doc_name": "1-大模型（LLMs）基础面.pdf",
    "source_doc_type": "pdf",
    "title_path": "1-大模型（LLMs）基础面.pdf > 大模型【 LLMs 】具有什么缺点? > 片段 9",
    "content": "1.需要消耗大量的计算资源和存储资源来训练和运行,这会增加经济和环境的负担。据估计,训练一个 GPT-3 模型需要消耗约 30 万美元,并产生约 284 吨二氧化碳排放;\n\n2.需要面对数据质量和安全性的问题,例如数据偏见、数据泄露、数据滥用等。这些问题可能会导致模型产生不准确或不道德的输出,并影响用户或社会的利益;\n\n3.需要考虑可解释性、可靠性、可持续性等方面的挑战,例如如何理解和控制模型的行为、如何保证模型的正确性和稳定性、如何平衡模型的效益和风险等。这些挑战需要多方面的研究和合作,以确保大模型能够健康地发展。",
    "order_index": 9,
    "char_count": 261
  }
]